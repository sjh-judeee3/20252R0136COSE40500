import cv2
import numpy as np
import torch
import time
import os
import sounddevice as sd
from scipy.io.wavfile import write
import sys 
import json

# run_multimodalì—ì„œ main_run í•¨ìˆ˜ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
# ì£¼ì˜: ì´ íŒŒì¼ê³¼ run_multimodal.pyëŠ” ê°™ì€ ë””ë ‰í† ë¦¬ì— ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
from run_multimodal import main_run 

# --- ì„¤ì •ê°’ ---
FPS = 30
CAPTURE_DURATION = 3.0  # 3ì´ˆ ë™ì•ˆ ìˆ˜ì–´ ë™ì‘ ìº¡ì²˜
OUTPUT_VIDEO_PATH = "captured_video.pt"  # PyTorch í…ì„œ íŒŒì¼
OUTPUT_AUDIO_PATH = "captured_audio.wav" # ì˜¤ë””ì˜¤ íŒŒì¼

# âš ï¸ ì‚¬ìš©ìì˜ ìµœì¢… í•™ìŠµëœ ëª¨ë¸ ë° í”„ë¡œí† íƒ€ì… ê²½ë¡œë¡œ ë³€ê²½í•˜ì„¸ìš”.
MODEL_PATH = "../model/slip_protonet_final.pth" 
PROTO_PATH = "prototypes.pt"

def print_status(message):
    """
    ëª¨ë“  ìƒíƒœ ë©”ì‹œì§€(ì§„í–‰ ìƒí™©, ì˜¤ë¥˜)ë¥¼ sys.stderrë¡œ ì¶œë ¥í•˜ì—¬ 
    stdout(ìµœì¢… í†µì—­ ê²°ê³¼)ê³¼ ë¶„ë¦¬í•˜ê³  Electronì˜ ì œìŠ¤ì²˜ ì˜¤ì‘ë™ì„ ë°©ì§€í•©ë‹ˆë‹¤.
    """
    print(message, file=sys.stderr)


def record_audio(filename, duration, samplerate=44100):
    """ìŒì„±ì„ ë…¹ìŒí•˜ì—¬ WAV íŒŒì¼ë¡œ ì €ì¥"""
    print_status(f"\nğŸ¤ {duration}ì´ˆ ë™ì•ˆ ìŒì„± ë…¹ìŒ ì‹œì‘...")
    # ì›í•˜ëŠ” ì…ë ¥ ì¥ì¹˜ê°€ ìˆë‹¤ë©´ MIC_DEVICE í™˜ê²½ë³€ìˆ˜ì— index(ìˆ«ì)ë‚˜ ì´ë¦„ì„ ë„£ì–´ ì‚¬ìš©
    mic_device = os.environ.get("MIC_DEVICE")
    if mic_device:
        try:
            mic_device = int(mic_device)
        except ValueError:
            pass  # ë¬¸ìì—´ ì´ë¦„ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        print_status(f"ğŸ™ï¸ ì…ë ¥ ì¥ì¹˜ ì§€ì •: {mic_device}")

    try:
        # ë…¹ìŒ ì‹œì‘
        recording = sd.rec(
            int(duration * samplerate),
            samplerate=samplerate,
            channels=1,
            dtype='int16',
            device=mic_device  # Noneì´ë©´ ê¸°ë³¸ ì…ë ¥ ì¥ì¹˜ ì‚¬ìš©
        )
        sd.wait()  # ë…¹ìŒì´ ëë‚  ë•Œê¹Œì§€ ëŒ€ê¸°

        # ì…ë ¥ ì‹ í˜¸ê°€ ì—†ëŠ” ê²½ìš°(ëª¨ë‘ 0) ë°”ë¡œ ì•Œë ¤ì¤Œ
        mean_amp = float(np.abs(recording).mean())
        if mean_amp < 1.0:
            print_status("âš ï¸ ë…¹ìŒëœ ì˜¤ë””ì˜¤ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ë§ˆì´í¬ ê¶Œí•œ/ì…ë ¥ ì¥ì¹˜ ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
            return False

        write(filename, samplerate, recording)
        print_status(f"âœ… ìŒì„± ë…¹ìŒ ì™„ë£Œ: {filename}")
        return True
    except Exception as e:
        print_status(f"âŒ ìŒì„± ë…¹ìŒ ì‹¤íŒ¨ (ë§ˆì´í¬ ì„¤ì • ë° 'sounddevice' ê¶Œí•œ í™•ì¸ í•„ìš”): {e}")
        return False


def main_capture():
    
    # 1. ì˜¤ë””ì˜¤ ë…¹ìŒì„ ì‹œì‘ (ë¹„ë””ì˜¤ ìº¡ì²˜ì™€ ìˆœì°¨ì ìœ¼ë¡œ ì§„í–‰)
    # ì˜¤ë””ì˜¤ ì‹¤íŒ¨ ì‹œ ë°”ë¡œ ì¢…ë£Œí•˜ì—¬ ë¹„ë””ì˜¤ ìì› ë‚­ë¹„ ë°©ì§€
    audio_success = record_audio(OUTPUT_AUDIO_PATH, CAPTURE_DURATION)
    if not audio_success:
        return None, None 

    # 2. ì¹´ë©”ë¼ ì´ˆê¸°í™” ë° ë¹„ë””ì˜¤ ìº¡ì²˜
    print_status("ğŸ¥ ì¹´ë©”ë¼ ì´ˆê¸°í™” ì¤‘...")
    cap = cv2.VideoCapture(0) # 0ë²ˆ ì¹´ë©”ë¼ ì¥ì¹˜ ì‹œë„
    
    if not cap.isOpened():
        print_status("âŒ ì¹´ë©”ë¼ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¥ì¹˜ ì—°ê²° ë° ê¶Œí•œ í™•ì¸ í•„ìš”.")
        return None, None
        
    TARGET_SIZE = (224, 224) 
    frames = []
    start_time = time.time()
    
    print_status(f"ğŸ¬ {CAPTURE_DURATION}ì´ˆ ë™ì•ˆ ìˆ˜ì–´ ë™ì‘ ìº¡ì²˜ ì‹œì‘...")

    while time.time() - start_time < CAPTURE_DURATION:
        ret, frame = cap.read()
        
        if not ret: 
            # í”„ë ˆì„ì„ ì½ì§€ ëª»í•˜ë©´ ë£¨í”„ë¥¼ ì¤‘ë‹¨í•˜ê³  ì˜¤ë¥˜ë¡œ ê°„ì£¼
            print_status("âš ï¸ ê²½ê³ : í”„ë ˆì„ì„ ì½ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì¹´ë©”ë¼ ì—°ê²°ì„ í™•ì¸í•˜ì„¸ìš”.")
            break
        
        # ìº¡ì²˜ëœ í”„ë ˆì„ ì²˜ë¦¬ (GUI ì½”ë“œ ì œê±°)
        processed_frame = cv2.flip(frame, 1)  # ì¢Œìš° ë°˜ì „
        processed_frame = cv2.resize(processed_frame, TARGET_SIZE)
        # BGR -> RGB ë° ì •ê·œí™” (0-255 -> 0-1). dtypeì€ ì´ë¯¸ float32ì…ë‹ˆë‹¤.
        processed_frame = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0
        frames.append(processed_frame)

    cap.release()
    cv2.destroyAllWindows()
    
    if not frames:
        print_status("âŒ ì¹˜ëª…ì  ì˜¤ë¥˜: ìº¡ì²˜ëœ í”„ë ˆì„ì´ 0ê°œì…ë‹ˆë‹¤.")
        return None, None
        
    print_status(f"âœ… ë¹„ë””ì˜¤ ìº¡ì²˜ ì™„ë£Œ. ì´ {len(frames)} í”„ë ˆì„ ì €ì¥ë¨.")


    # 3. PyTorch í…ì„œë¡œ ë³€í™˜ ë° ì €ì¥
    video_np = np.stack(frames) # (T, H, W, 3)
    
    # ğŸš¨ğŸš¨ ìµœì¢… ìˆ˜ì •: dtypeì„ ëª…ì‹œí•˜ê³  contiguous arrayë¡œ ë³€í™˜í•œ í›„ torch.from_numpy í˜¸ì¶œ ğŸš¨ğŸš¨
    # ì´ë ‡ê²Œ í•˜ë©´ ê±°ì˜ ëª¨ë“  í™˜ê²½ì—ì„œ í˜¸í™˜ì„± ë¬¸ì œê°€ í•´ê²°ë©ë‹ˆë‹¤.
    video_np_final = np.ascontiguousarray(video_np, dtype=np.float32)
    video_tensor = torch.from_numpy(video_np_final).permute(0, 3, 1, 2) # (T, 3, H, W)
    
    torch.save(video_tensor, OUTPUT_VIDEO_PATH)
    print_status(f"âœ… ë¹„ë””ì˜¤ í…ì„œ ì €ì¥ ì™„ë£Œ: {OUTPUT_VIDEO_PATH}")

    return OUTPUT_VIDEO_PATH, OUTPUT_AUDIO_PATH

if __name__ == '__main__':
    video_file, audio_file = main_capture()
    
    # -------------------------------------------------------------
    # ğŸŒŸğŸŒŸğŸŒŸ í†µí•© ì‹¤í–‰: ìº¡ì²˜ ì™„ë£Œ í›„ run_multimodalì˜ main_run í˜¸ì¶œ ğŸŒŸğŸŒŸğŸŒŸ
    # -------------------------------------------------------------
    if video_file and audio_file:
        
        # motionCapture.pyê°€ run_multimodal.pyì˜ main_run í•¨ìˆ˜ë¥¼ ì§ì ‘ í˜¸ì¶œí•©ë‹ˆë‹¤.
        llm_response = main_run(
            MODEL_PATH, PROTO_PATH, video_file, audio_file
        )
        print(json.dumps({
            "type": "LLM_RESPONSE",
            "data": llm_response
        }))
    else:
        print_status("âŒ ìº¡ì²˜ ì˜¤ë¥˜ë¡œ ì¸í•´ ì¶”ë¡ ì„ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
